{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd25c871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a39a13b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dobar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04569379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in cnn_dailymail : ['article', 'highlights', 'id']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\n",
    "print(f\"Features in cnn_dailymail : {dataset['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74c84465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Article (excerpt of 500 characters, total length: 4051):\n",
      "\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most s\n",
      "\n",
      "Summary (length: 281):\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[\"train\"][1]\n",
    "print(f\"\"\"\n",
    "Article (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n",
    "\"\"\")\n",
    "print(sample[\"article\"][:500])\n",
    "print(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\n",
    "print(sample[\"highlights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc91ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = dataset[\"train\"][1][\"article\"][:1000]\n",
    "\n",
    "# We'll collect the generated summaries of each model in a dictionary\n",
    "summaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995cd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_summary_three_sent(text):\n",
    "    return \"\\n\".join(sent_tokenize(text)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47cd5656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\\nHere, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial.\\nMIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries['baseline'] = baseline_summary_three_sent(sample_text)\n",
    "\n",
    "summaries['baseline']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2f1839",
   "metadata": {},
   "source": [
    "# GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd9fdf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "pipe = pipeline('text-generation', model = 'gpt2-medium' )\n",
    "\n",
    "gpt2_query = sample_text + \"\\nTL;DR:\\n\"\n",
    "\n",
    "pipe_out = pipe(gpt2_query, max_length = 512, clean_up_tokenization_spaces = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454f3bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Editor\\'s note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O\\'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they\\'re ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow dir\\nTL;DR:\\nThe Eighth Amendment of the U.S. Constitution protects citizens from unreasonable search & seizure. This is what Leifman said in a 2012 hearing before Florida\\'s Florida Supreme Court when questioned by judge Patricia Smith to address the problem of mentally ill people being arrested by police. That hearing occurred three months after a local judge in Miami ordered the state of Florida to stop using police officers as stand-by for all mentally ill people, something the U.S. Supreme Court ruled as unconstitutional a month before. \"I think I\\'ve made it clear that the Fourth Amendment prohibits police officers from using non-law enforcement officers,\" Leifman said. \"And I believe in common law -- and I\\'m proud of this fact -- in common law, it is unlawful for police officers to serve as stand-bys for a mentally ill person.\" He noted that the Fourth Amendment was intended to stop a person suffering involuntary manslaughter when his or her own death threatened to occur as a result of their inability to protect themselves. \"It was meant to stop a person coming into the presence of the police and having someone else be there in the presence of them,\" he said, \"so I don\\'t think that\\'s very helpful in addressing mentally ill people who have been arrested.\"\\nPosted by Jacob at 4:33 PM'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7d8756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Eighth Amendment of the U.S. Constitution protects citizens from unreasonable search & seizure. This is what Leifman said in a 2012 hearing before Florida\\'s Florida Supreme Court when questioned by judge Patricia Smith to address the problem of mentally ill people being arrested by police. That hearing occurred three months after a local judge in Miami ordered the state of Florida to stop using police officers as stand-by for all mentally ill people, something the U.S. Supreme Court ruled as unconstitutional a month before. \"I think I\\'ve made it clear that the Fourth Amendment prohibits police officers from using non-law enforcement officers,\" Leifman said. \"And I believe in common law -- and I\\'m proud of this fact -- in common law, it is unlawful for police officers to serve as stand-bys for a mentally ill person.\" He noted that the Fourth Amendment was intended to stop a person suffering involuntary manslaughter when his or her own death threatened to occur as a result of their inability to protect themselves. \"It was meant to stop a person coming into the presence of the police and having someone else be there in the presence of them,\" he said, \"so I don\\'t think that\\'s very helpful in addressing mentally ill people who have been arrested.\"\\nPosted by Jacob at 4:33 PM'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out[0][\"generated_text\"][len(gpt2_query) :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af169fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries['gpt2'] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fd1c8",
   "metadata": {},
   "source": [
    "# T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d2d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline('summarization', model = 't5-small' )\n",
    "\n",
    "pipe_out = pipe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31793e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court . most often, they face drug charges or charges of assaulting an officer . mentally ill people become more paranoid, delusional, and less likely to follow dir .\"}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88bd8e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries['t5'] = 'n'.join(sent_tokenize(pipe_out[0]['summary_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126e6a2",
   "metadata": {},
   "source": [
    "# BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff72a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "pipe_out = pipe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e32f3cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Miami-Dade pretrial detention facility is dubbed the \"forgotten floor\" Here, inmates with the most severe mental illnesses are incarcerated. Most often, they face drug charges or charges of assaulting an officer. Judge Steven Leifman says the arrests often result from confrontations with police.'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c2e5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c00928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Miami-Dade pretrial detention facility is dubbed the \"forgotten floor\" Here, inmates with the most severe mental illnesses are incarcerated.\\nMost often, they face drug charges or charges of assaulting an officer.\\nJudge Steven Leifman says the arrests often result from confrontations with police.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[\"bart\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bf3a8a",
   "metadata": {},
   "source": [
    "# PEGASUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c8bc7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline('summarization', model=\"google/pegasus-cnn_dailymail\"  )\n",
    "\n",
    "pipe_out = pipe(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40db4478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Mentally ill inmates are housed on the \"forgotten floor\" of a Miami jail .<n>Judge Steven Leifman says the charges are usually \"avoidable felonies\"<n>He says the arrests often result from confrontations with police .<n>Mentally ill people often won\\'t do what they\\'re told when police arrive on the scene .'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c041b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e11bc",
   "metadata": {},
   "source": [
    "Compare different summarizes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9303070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROUND TRUTH\n",
      "Mentally ill inmates in Miami are housed on the \"forgotten floor\"\n",
      "Judge Steven Leifman says most are there as a result of \"avoidable felonies\"\n",
      "While CNN tours facility, patient shouts: \"I am the son of the president\"\n",
      "Leifman says the system is unjust and he's fighting for change .\n",
      "BASELINE\n",
      "Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events.\n",
      "Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial.\n",
      "MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\"\n",
      "GPT2\n",
      "The Eighth Amendment of the U.S. Constitution protects citizens from unreasonable search & seizure.\n",
      "This is what Leifman said in a 2012 hearing before Florida's Florida Supreme Court when questioned by judge Patricia Smith to address the problem of mentally ill people being arrested by police.\n",
      "That hearing occurred three months after a local judge in Miami ordered the state of Florida to stop using police officers as stand-by for all mentally ill people, something the U.S. Supreme Court ruled as unconstitutional a month before.\n",
      "\"I think I've made it clear that the Fourth Amendment prohibits police officers from using non-law enforcement officers,\" Leifman said.\n",
      "\"And I believe in common law -- and I'm proud of this fact -- in common law, it is unlawful for police officers to serve as stand-bys for a mentally ill person.\"\n",
      "He noted that the Fourth Amendment was intended to stop a person suffering involuntary manslaughter when his or her own death threatened to occur as a result of their inability to protect themselves.\n",
      "\"It was meant to stop a person coming into the presence of the police and having someone else be there in the presence of them,\" he said, \"so I don't think that's very helpful in addressing mentally ill people who have been arrested.\"\n",
      "Posted by Jacob at 4:33 PM\n",
      "T5\n",
      "inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court .nmost often, they face drug charges or charges of assaulting an officer .nmentally ill people become more paranoid, delusional, and less likely to follow dir .\n",
      "BART\n",
      "Miami-Dade pretrial detention facility is dubbed the \"forgotten floor\" Here, inmates with the most severe mental illnesses are incarcerated.\n",
      "Most often, they face drug charges or charges of assaulting an officer.\n",
      "Judge Steven Leifman says the arrests often result from confrontations with police.\n",
      "PEGASUS\n",
      "Mentally ill inmates are housed on the \"forgotten floor\" of a Miami jail.\n",
      "Judge Steven Leifman says the charges are usually \"avoidable felonies\"<n>He says the arrests often result from confrontations with police.\n",
      "Mentally ill people often won't do what they're told when police arrive on the scene .\n"
     ]
    }
   ],
   "source": [
    "print(\"GROUND TRUTH\")\n",
    "\n",
    "print(dataset['train'][1]['highlights'])\n",
    "\n",
    "\n",
    "for model_name in summaries:\n",
    "    print(model_name.upper())\n",
    "    print(summaries[model_name])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed13b0",
   "metadata": {},
   "source": [
    "# SacreBLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b337e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dobar\\AppData\\Local\\Temp\\ipykernel_10864\\3278324519.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bleu_metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fd75b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>18.73841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>[27, 14, 10, 6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>totals</th>\n",
       "      <td>[67, 66, 65, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precisions</th>\n",
       "      <td>[40.298507462686565, 21.21212121212121, 15.384...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bp</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sys_len</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ref_len</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>[40.3, 21.21, 15.38, 9.38]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Value\n",
       "score                                                18.73841\n",
       "counts                                        [27, 14, 10, 6]\n",
       "totals                                       [67, 66, 65, 64]\n",
       "precisions  [40.298507462686565, 21.21212121212121, 15.384...\n",
       "bp                                                        1.0\n",
       "sys_len                                                    67\n",
       "ref_len                                                    57\n",
       "precision                          [40.3, 21.21, 15.38, 9.38]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_metric.add(prediction = [summaries[\"pegasus\"]], reference = [dataset['train'][1]['highlights'] ])\n",
    "\n",
    "results = bleu_metric.compute(smooth_method = 'floor', smooth_value = 0 )\n",
    "\n",
    "results['precision'] = [np.round(p , 2) for p in results['precisions'] ]\n",
    "\n",
    "pd.DataFrame.from_dict(results, orient = 'index', columns = ['Value'] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1864bc8",
   "metadata": {},
   "source": [
    "# ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f43611af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "544ab8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_dict  {'rouge1': 0.365079365079365, 'rouge2': 0.14516129032258066, 'rougeL': 0.20634920634920634, 'rougeLsum': 0.2857142857142857}\n",
      "rouge_dict  {'rouge1': 0.16666666666666666, 'rouge2': 0.043795620437956206, 'rougeL': 0.11594202898550726, 'rougeLsum': 0.15217391304347824}\n",
      "rouge_dict  {'rouge1': 0.1758241758241758, 'rouge2': 0.0, 'rougeL': 0.13186813186813187, 'rougeLsum': 0.15384615384615383}\n",
      "rouge_dict  {'rouge1': 0.3655913978494624, 'rouge2': 0.13186813186813184, 'rougeL': 0.2150537634408602, 'rougeLsum': 0.3225806451612903}\n",
      "rouge_dict  {'rouge1': 0.5, 'rouge2': 0.24489795918367346, 'rougeL': 0.36000000000000004, 'rougeLsum': 0.46}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.145161</td>\n",
       "      <td>0.206349</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt2</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.043796</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t5</th>\n",
       "      <td>0.175824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bart</th>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.215054</td>\n",
       "      <td>0.322581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pegasus</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.244898</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.365079  0.145161  0.206349   0.285714\n",
       "gpt2      0.166667  0.043796  0.115942   0.152174\n",
       "t5        0.175824  0.000000  0.131868   0.153846\n",
       "bart      0.365591  0.131868  0.215054   0.322581\n",
       "pegasus   0.500000  0.244898  0.360000   0.460000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "\n",
    "reference = dataset['train'][1]['highlights']\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name in summaries:\n",
    "    rouge_metric.add(prediction = summaries[model_name], reference = reference )\n",
    "    score = rouge_metric.compute()\n",
    "    rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "    print('rouge_dict ', rouge_dict )\n",
    "    records.append(rouge_dict)\n",
    "\n",
    "pd.DataFrame.from_records(records, index = summaries.keys() )\n",
    "\n",
    "#accuracy = accuracy_score(records[1],records[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a02582a",
   "metadata": {},
   "source": [
    "# Evaluationg on the TEST set of the CNN/DailyMail Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f477c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_on_baseline_test_ds(dataset, metric, column_text = 'article', column_summary = 'highlights' ):\n",
    "    \"\"\"\n",
    "    This function calculates a specified metric on a baseline test dataset for a Natural Language Processing (NLP) task.\n",
    "    It assumes the task is a text summarization task, where the goal is to generate a summary (e.g., highlights) from a text (e.g., article).\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pandas.DataFrame): The test dataset. It should contain a column for the text and a column for the true summary.\n",
    "    metric (datasets.Metric): The metric to calculate. This should be a metric object from the Hugging Face datasets library.\n",
    "    column_text (str, optional): The name of the column in the dataset that contains the text. Defaults to 'article'.\n",
    "    column_summary (str, optional): The name of the column in the dataset that contains the true summary. Defaults to 'highlights'.\n",
    "\n",
    "    Returns:\n",
    "    score (float): The calculated score of the metric on the test dataset.\n",
    "    \"\"\"\n",
    "    summaries = [baseline_summary_three_sent(text) for text in dataset[column_text] ]\n",
    "\n",
    "    metric.add_batch(predictions = summaries, references = dataset[column_summary] )\n",
    "\n",
    "    score = metric.compute()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "489b84af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.253995</td>\n",
       "      <td>0.100642</td>\n",
       "      <td>0.165754</td>\n",
       "      <td>0.231571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rouge1    rouge2    rougeL  rougeLsum\n",
       "baseline  0.253995  0.100642  0.165754   0.231571"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sampled = dataset['train'].shuffle(seed = 42).select(range(1000))\n",
    "\n",
    "score = calculate_metric_on_baseline_test_ds(test_sampled, rouge_metric )\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "pd.DataFrame.from_dict(rouge_dict, orient = 'index' , columns = ['baseline'] ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ba848",
   "metadata": {},
   "source": [
    "# Strategy to calculate the ROUGE Metric on test dataset for the other Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7108f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    \"\"\"split the dataset into smaller batches that we can process simultaneously\n",
    "    Yield successive batch-sized chunks from list_of_elements.\n",
    "    \n",
    "    Generator function to yield successive batch-sized chunks from list_of_elements.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_elements (list): List of elements to be divided into chunks.\n",
    "    batch_size (int): The size of each chunk.\n",
    "\n",
    "    Yields:\n",
    "    list: Batch-sized chunk from list_of_elements.\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, \n",
    "                               batch_size=16, device=device, \n",
    "                               column_text=\"article\", \n",
    "                               column_summary=\"highlights\"):\n",
    "    \"\"\"\n",
    "    Function to calculate a specified metric on a test dataset for a Natural Language Processing (NLP) task.\n",
    "    It assumes the task is a text summarization task, where the goal is to generate a summary from a text.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (pandas.DataFrame): The test dataset. It should contain a column for the text and a column for the true summary.\n",
    "    metric (datasets.Metric): The metric to calculate. This should be a metric object from the Hugging Face datasets library.\n",
    "    model (transformers.PreTrainedModel): The transformer model to use for text generation.\n",
    "    tokenizer (transformers.PreTrainedTokenizer): The tokenizer corresponding to the model.\n",
    "    batch_size (int, optional): The size of the batches to use for processing. Defaults to 16.\n",
    "    device (str, optional): The device to run the model on. Defaults to the output of torch.cuda.is_available().\n",
    "    column_text (str, optional): The name of the column in the dataset that contains the text. Defaults to 'article'.\n",
    "    column_summary (str, optional): The name of the column in the dataset that contains the true summary. Defaults to 'highlights'.\n",
    "\n",
    "    Returns:\n",
    "    score (float): The calculated score of the metric on the test dataset.\n",
    "    \"\"\"\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "        \n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True, \n",
    "                        padding=\"max_length\", return_tensors=\"pt\")\n",
    "        \n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                         attention_mask=inputs[\"attention_mask\"].to(device), \n",
    "                         length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "        \n",
    "        # Finally, we decode the generated texts, \n",
    "        # replace the <n> token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, \n",
    "                                clean_up_tokenization_spaces=True) \n",
    "               for s in summaries]      \n",
    "        \n",
    "        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n",
    "        \n",
    "        \n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "        \n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n",
    "\n",
    "score = calculate_metric_on_test_ds(test_sampled, rouge_metric, \n",
    "                                   model_pegasus, tokenizer, batch_size=8)\n",
    "\n",
    "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
    "\n",
    "# At the end, we compute and return the ROUGE scores.\n",
    "pd.DataFrame(rouge_dict, index=[\"pegasus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111243a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c06e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
