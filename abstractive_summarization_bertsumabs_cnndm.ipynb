{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstractive Summarization using BertSumAbs on CNN/DailyMails Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to fine tune BERT for abstractive text summarization. Utility functions and classes in the NLP Best Practices repo are used to facilitate data preprocessing, model training, model scoring, result postprocessing, and model evaluation.\n",
    "\n",
    "### Abstractive Summarization\n",
    "Abstractive summarization is the task of taking an input text and summarizing its content in a shorter output text. In contrast to extractive summarization, abstractive summarization doesn't take sentences directly from the input text, instead, rephrases the input text.\n",
    "\n",
    "### BertSumAbs\n",
    "\n",
    "BertSumAbs refers to an BERT-based abstractive summarization algorithm  in [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345) with [published examples](https://github.com/nlpyang/PreSumm). It uses the pretrained BERT model as encoder and finetune both encoder and decoder on a specific labeled summarization dataset like [CNN/DM dataset](https://github.com/harvardnlp/sent-summary). \n",
    "\n",
    "The figure below shows the comparison of architecture of the original BERT model (left) and BERTSUM (right), which BertSumAbs is built upon. For BERTSUM, a input document is split into sentences, and [CLS] and [SEP] tokens are inserted before and after each sentence. This resulting sequence is followed by the summation of three kinds of embeddings for each token before feeding into the transformer layers. The positional embedding used in BertSumAbs enables input length of more than 512, which is the  maximum input length for BERT model. \n",
    "\n",
    "It should be noted that the architecture only shows the encoder part. For decoder, BertSumAbs also uses a transformer with multiple layers and random initialization. As pretrained weights are used in the encoder, there is a mismatch in encoder and decoder which may result in unstable finetuning. Therefore, in fine tuning, BertSumAbs uses seperate optimizers for encoder and decoder, each uses its own scheduling. In text generation, techniques like trigram blocking and beam search can be used to improve model accuracy.\n",
    "<img src=\"https://nlpbp.blob.core.windows.net/images/BertForSummarization.PNG\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "It's recommended to run this notebook on GPU machines as it's very computationally intensive. Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of steps. If QUICK_RUN = False, the notebook takes about 5 hours to run on a VM with 4 16GB NVIDIA V100 GPUs. Finetuning costs around 1.5 hours and inferecing costs around 3.5 hour.  Better performance can be achieved by increasing the MAX_STEPS.\n",
    "\n",
    "* **ROUGE Evalation**: To run rouge evaluation, please refer to the section of compute_rouge_perl in [summarization_evaluation.ipynb](./summarization_evaluation.ipynb) for setup.\n",
    "\n",
    "* **Distributed Training**:\n",
    "Please note that the jupyter notebook only allows to use pytorch [DataParallel](https://pytorch.org/docs/master/nn.html#dataparallel). Faster speed and larger batch size can be achieved with pytorch [DistributedDataParallel](https://pytorch.org/docs/master/notes/ddp.html)(DDP). Script [abstractive_summarization_bertsum_cnndm_distributed_train.py](./abstractive_summarization_bertsum_cnndm_distributed_train.py) shows an example of how to use DDP.\n",
    "\n",
    "* **Mixed Precision Training**:\n",
    "Please note that by default this notebook doesn't use mixed precision training. Faster speed and larger batch size can be achieved when you set FP16 to True. Refer to  https://nvidia.github.io/apex and https://github.com/nvidia/apex) for details to use mixed precision training. Check the GPU model on your machine to see if it allows mixed precision training. Please also note that mixed precision inferencing is also enabled in the prediciton utility function. When you use mixed precision training and/or inferencing, the model performance can be slightly worse than the full precision mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "import torch\n",
    "\n",
    "nlp_path = os.path.abspath(\"../../\")\n",
    "if nlp_path not in sys.path:\n",
    "    sys.path.insert(0, nlp_path)\n",
    "\n",
    "from utils_nlp.models.transformers.abstractive_summarization_bertsum import (\n",
    "    BertSumAbs,\n",
    "    BertSumAbsProcessor,\n",
    ")\n",
    "\n",
    "from utils_nlp.dataset.cnndm import CNNDMSummarizationDataset\n",
    "from utils_nlp.eval import compute_rouge_python\n",
    "\n",
    "from utils_nlp.models.transformers.datasets import SummarizationDataset\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import scrapbook as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we used for this notebook is CNN/DM dataset which contains the documents and accompanying questions from the news articles of CNN and Daily mail. The highlights in each article are used as summary. The dataset consits of ~289K training examples, ~11K valiation examples and ~11K test examples. The length of the news articles is 781 tokens on average and the summaries are of 3.75 sentences and 56 tokens on average.\n",
    "\n",
    "The significant part of data preprocessing only involve splitting the input document into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data path used to save the downloaded data file\n",
    "DATA_PATH = TemporaryDirectory().name\n",
    "# The number of lines at the head of data file used for preprocessing. -1 means all the lines.\n",
    "TOP_N = 100\n",
    "if not QUICK_RUN:\n",
    "    TOP_N = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 489k/489k [00:07<00:00, 62.5kKB/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = CNNDMSummarizationDataset(\n",
    "    top_n=TOP_N, local_cache_path=DATA_PATH, prepare_extractive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utils_nlp.models.transformers.datasets.SummarizationDataset"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# notebook parameters\n",
    "# the cache path\n",
    "CACHE_PATH = TemporaryDirectory().name\n",
    "\n",
    "# model parameters\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "MAX_POS = 768\n",
    "MAX_SOURCE_SEQ_LENGTH = 640\n",
    "MAX_TARGET_SEQ_LENGTH = 140\n",
    "\n",
    "# mixed precision setting. To enable mixed precision training, follow instructions in SETUP.md.\n",
    "FP16 = False\n",
    "if FP16:\n",
    "    FP16_OPT_LEVEL = \"O2\"\n",
    "\n",
    "# fine-tuning parameters\n",
    "# batch size, unit is the number of tokens\n",
    "BATCH_SIZE_PER_GPU = 1\n",
    "\n",
    "\n",
    "# GPU used for training\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "if NUM_GPUS > 0:\n",
    "    BATCH_SIZE = NUM_GPUS * BATCH_SIZE_PER_GPU\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "# Learning rate\n",
    "LEARNING_RATE_BERT = 5e-4 / 2.0\n",
    "LEARNING_RATE_DEC = 0.05 / 2.0\n",
    "\n",
    "\n",
    "# How often the statistics reports show up in training, unit is step.\n",
    "REPORT_EVERY = 10\n",
    "SAVE_EVERY = 500\n",
    "\n",
    "# total number of steps for training\n",
    "MAX_STEPS = 1e3\n",
    "\n",
    "if not QUICK_RUN:\n",
    "    MAX_STEPS = 5e3\n",
    "\n",
    "WARMUP_STEPS_BERT = 2000\n",
    "WARMUP_STEPS_DEC = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d0aceb9fd6a4bb6905397a46cc9adce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c8af76ada54ba7915edda498f75371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "885b6c2ab6864292b579af1c350e4ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# processor which contains the colloate function to load the preprocessed data\n",
    "processor = BertSumAbsProcessor(cache_dir=CACHE_PATH, max_src_len=MAX_SOURCE_SEQ_LENGTH, max_tgt_len=MAX_TARGET_SEQ_LENGTH)\n",
    "# summarizer\n",
    "summarizer = BertSumAbs(\n",
    "    processor, cache_dir=CACHE_PATH, max_pos_length=MAX_POS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE_PER_GPU*NUM_GPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:37<00:38,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:18:40, average loss: 10.913145, time duration: 37.846008,\n",
      "                            number of examples in current reporting: 50, step 50\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:19:18, average loss: 6.579080, time duration: 37.992080,\n",
      "                            number of examples in current reporting: 50, step 100\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:19:56, average loss: 5.378111, time duration: 38.216206,\n",
      "                            number of examples in current reporting: 50, step 150\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:16<00:00,  1.29it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:20:34, average loss: 5.155963, time duration: 38.352587,\n",
      "                            number of examples in current reporting: 50, step 200\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:21:13, average loss: 4.821997, time duration: 38.510111,\n",
      "                            number of examples in current reporting: 50, step 250\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:21:51, average loss: 4.793069, time duration: 38.494815,\n",
      "                            number of examples in current reporting: 50, step 300\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:22:30, average loss: 4.305557, time duration: 38.584101,\n",
      "                            number of examples in current reporting: 50, step 350\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:23:08, average loss: 4.316215, time duration: 38.579625,\n",
      "                            number of examples in current reporting: 50, step 400\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:23:47, average loss: 3.650458, time duration: 38.696634,\n",
      "                            number of examples in current reporting: 50, step 450\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  99%|█████████▉| 99/100 [01:16<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:24:26, average loss: 3.871134, time duration: 38.610011,\n",
      "                            number of examples in current reporting: 50, step 500\n",
      "                            out of total 1000\n",
      "saving through pytorch to /tmp/tmpxrkfobre/fine_tuned/bertsumabs.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:19<00:00,  1.43s/it]\n",
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:25:07, average loss: 3.101695, time duration: 40.749707,\n",
      "                            number of examples in current reporting: 50, step 550\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:25:45, average loss: 3.153789, time duration: 38.725723,\n",
      "                            number of examples in current reporting: 50, step 600\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:26:24, average loss: 2.401619, time duration: 38.709995,\n",
      "                            number of examples in current reporting: 50, step 650\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:27:03, average loss: 2.689615, time duration: 38.607277,\n",
      "                            number of examples in current reporting: 50, step 700\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:27:41, average loss: 1.909993, time duration: 38.629452,\n",
      "                            number of examples in current reporting: 50, step 750\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.28it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:28:20, average loss: 2.174074, time duration: 38.523073,\n",
      "                            number of examples in current reporting: 50, step 800\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:28:58, average loss: 1.527047, time duration: 38.659285,\n",
      "                            number of examples in current reporting: 50, step 850\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:17<00:00,  1.30it/s]\n",
      "Iteration:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:29:37, average loss: 1.821635, time duration: 38.598318,\n",
      "                            number of examples in current reporting: 50, step 900\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  50%|█████     | 50/100 [00:38<00:38,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:30:16, average loss: 1.403594, time duration: 38.629701,\n",
      "                            number of examples in current reporting: 50, step 950\n",
      "                            out of total 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  99%|█████████▉| 99/100 [01:16<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp: 24/07/2020 05:30:54, average loss: 1.675590, time duration: 38.505846,\n",
      "                            number of examples in current reporting: 50, step 1000\n",
      "                            out of total 1000\n",
      "saving through pytorch to /tmp/tmpxrkfobre/fine_tuned/bertsumabs.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 100/100 [01:54<00:00, 11.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving through pytorch to /tmp/tmpxrkfobre/fine_tuned/bertsumabs.pt\n"
     ]
    }
   ],
   "source": [
    "summarizer.fit(\n",
    "    train_dataset,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    learning_rate_bert=LEARNING_RATE_BERT,\n",
    "    learning_rate_dec=LEARNING_RATE_DEC,\n",
    "    warmup_steps_bert=WARMUP_STEPS_BERT,\n",
    "    warmup_steps_dec=WARMUP_STEPS_DEC,\n",
    "    save_every=SAVE_EVERY,\n",
    "    report_every=REPORT_EVERY * 5,\n",
    "    fp16=FP16,\n",
    "    # checkpoint=\"saved checkpoint path\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpxrkfobre\n",
      "saving through pytorch to /tmp/tmpxrkfobre/bertsumabs.pt\n"
     ]
    }
   ],
   "source": [
    "summarizer.save_model(MAX_STEPS, os.path.join(CACHE_PATH, \"bertsumabs.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "To run rouge evaluation, please refer to the section of compute_rouge_perl in [summarization_evaluation.ipynb](summarization_evaluation.ipynb) for setup.\n",
    "For the settings in this notebook with QUICK_RUN=False, you should get ROUGE scores close to the following numbers: <br />\n",
    "``\n",
    "{'rouge-1': {'f': 0.34819639878321873,\n",
    "             'p': 0.39977932634737307,\n",
    "             'r': 0.34429079596863604},\n",
    " 'rouge-2': {'f': 0.13919271352557894,\n",
    "             'p': 0.16129965067780644,\n",
    "             'r': 0.1372938054050938},\n",
    " 'rouge-l': {'f': 0.2313282318854973,\n",
    "             'p': 0.26664667422849747,\n",
    "             'r': 0.22850294283399628}}\n",
    " ``\n",
    " \n",
    " Better performance can be achieved by increasing the MAX_STEPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating summary:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length is 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summary: 100%|██████████| 32/32 [02:18<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "TEST_TOP_N = 32\n",
    "if not QUICK_RUN:\n",
    "    TEST_TOP_N = len(test_dataset)\n",
    "\n",
    "if NUM_GPUS:\n",
    "    BATCH_SIZE = NUM_GPUS * BATCH_SIZE_PER_GPU\n",
    "else:\n",
    "    BATCH_SIZE = 1\n",
    "    \n",
    "shortened_dataset = test_dataset.shorten(top_n=TEST_TOP_N)\n",
    "src = shortened_dataset.get_source()\n",
    "reference_summaries = [\" \".join(t).rstrip(\"\\n\") for t in shortened_dataset.get_target()]\n",
    "generated_summaries = summarizer.predict(\n",
    "    shortened_dataset, batch_size=BATCH_SIZE, num_gpus=NUM_GPUS\n",
    ")\n",
    "assert len(generated_summaries) == len(reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['( cnn ) the palestinian authority officially became the 123rd member of the international criminal court on wednesday , a step that gives the court jurisdiction over alleged crimes in palestinian territories .',\n",
       " 'the formal accession was marked with a ceremony at the hague , in the netherlands , where the court is based .',\n",
       " 'the palestinians signed the icc \\'s founding rome statute in january , when they also accepted its jurisdiction over alleged crimes committed \" in the occupied palestinian territory , including east jerusalem , since june 13 , 2014 . \"',\n",
       " 'later that month , the icc opened a preliminary examination into the situation in palestinian territories , paving the way for possible war crimes investigations against israelis .',\n",
       " 'as members of the court , palestinians may be subject to counter-charges as well .',\n",
       " \"israel and the united states , neither of which is an icc member , opposed the palestinians ' efforts to join the body .\",\n",
       " 'but palestinian foreign minister riad al-malki , speaking at wednesday \\'s ceremony , said it was a move toward greater justice . \"',\n",
       " 'as palestine formally becomes a state party to the rome statute today , the world is also a step closer to ending a long era of impunity and injustice , \" he said , according to an icc news release . \"',\n",
       " 'indeed , today brings us closer to our shared goals of justice and peace . \"',\n",
       " 'judge kuniko ozaki , a vice president of the icc , said acceding to the treaty was just the first step for the palestinians . \"',\n",
       " 'as the rome statute today enters into force for the state of palestine , palestine acquires all the rights as well as responsibilities that come with being a state party to the statute .',\n",
       " 'these are substantive commitments , which can not be taken lightly , \" she said .',\n",
       " 'rights group human rights watch welcomed the development . \"',\n",
       " 'governments seeking to penalize palestine for joining the icc should immediately end their pressure , and countries that support universal acceptance of the court \\'s treaty should speak out to welcome its membership , \" said balkees jarrah , international justice counsel for the group . \"',\n",
       " 'what \\'s objectionable is the attempts to undermine international justice , not palestine \\'s decision to join a treaty to which over 100 countries around the world are members . \"',\n",
       " 'in january , when the preliminary icc examination was opened , israeli prime minister benjamin netanyahu described it as an outrage , saying the court was overstepping its boundaries .',\n",
       " 'the united states also said it \" strongly \" disagreed with the court \\'s decision . \"',\n",
       " 'as we have said repeatedly , we do not believe that palestine is a state and therefore we do not believe that it is eligible to join the icc , \" the state department said in a statement .',\n",
       " 'it urged the warring sides to resolve their differences through direct negotiations . \"',\n",
       " 'we will continue to oppose actions against israel at the icc as counterproductive to the cause of peace , \" it said .',\n",
       " 'but the icc begs to differ with the definition of a state for its purposes and refers to the territories as \" palestine . \"',\n",
       " 'while a preliminary examination is not a formal investigation , it allows the court to review evidence and determine whether to investigate suspects on both sides .',\n",
       " 'prosecutor fatou bensouda said her office would \" conduct its analysis in full independence and impartiality . \"',\n",
       " 'the war between israel and hamas militants in gaza last summer left more than 2,000 people dead .',\n",
       " 'the inquiry will include alleged war crimes committed since june .',\n",
       " 'the international criminal court was set up in 2002 to prosecute genocide , crimes against humanity and war crimes .',\n",
       " \"cnn 's vasco cotovio , kareem khadder and faith karimi contributed to this report .\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sheikh mohammed bin bin bin isa al khalifa steers bahrain \\' s economic growth .   al khalifa steer steers the finance sector in human capital \" human capital calls on world to unite to resolve nuclear dispute with iran . s economic economic growth in human rights in u . s . s terrorists terrorists . s crisis has reduced its \" human rights . s s terrorists . identity . s .           .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' membership gives the icc jurisdiction over alleged crimes committed in palestinian territories since last june .    israel and the united states opposed the move , which could open the door to war crimes investigations against israelis .  '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_summaries[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of candidates: 32\n",
      "Number of references: 32\n",
      "{'rouge-1': {'f': 0.10620883021910923,\n",
      "             'p': 0.09713175314581397,\n",
      "             'r': 0.14127707280011492},\n",
      " 'rouge-2': {'f': 0.00509789676653209,\n",
      "             'p': 0.004414273530780389,\n",
      "             'r': 0.006830523949090126},\n",
      " 'rouge-l': {'f': 0.07381271577399373,\n",
      "             'p': 0.06835281467114104,\n",
      "             'r': 0.09708573736933927}}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = compute_rouge_python(cand=generated_summaries, ref=reference_summaries)\n",
    "pprint.pprint(rouge_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "sb.glue(\"rouge_2_f_score\", rouge_scores['rouge-2']['f'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on a single input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"\"\"\n",
    "But under the new rule, set to be announced in the next 48 hours, Border Patrol agents would immediately return anyone to Mexico — without any detainment and without any due process — who attempts to cross the southwestern border between the legal ports of entry. The person would not be held for any length of time in an American facility.\n",
    "\n",
    "Although they advised that details could change before the announcement, administration officials said the measure was needed to avert what they fear could be a systemwide outbreak of the coronavirus inside detention facilities along the border. Such an outbreak could spread quickly through the immigrant population and could infect large numbers of Border Patrol agents, leaving the southwestern border defenses weakened, the officials argued.\n",
    "The Trump administration plans to immediately turn back all asylum seekers and other foreigners attempting to enter the United States from Mexico illegally, saying the nation cannot risk allowing the coronavirus to spread through detention facilities and Border Patrol agents, four administration officials said.\n",
    "The administration officials said the ports of entry would remain open to American citizens, green-card holders and foreigners with proper documentation. Some foreigners would be blocked, including Europeans currently subject to earlier travel restrictions imposed by the administration. The points of entry will also be open to commercial traffic.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating summary:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length is 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating summary: 100%|██████████| 1/1 [00:04<00:00,  4.47s/it]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SummarizationDataset(\n",
    "    None, source=[source], source_preprocessing=[tokenize.sent_tokenize],\n",
    ")\n",
    "generated_summaries = summarizer.predict(test_dataset, batch_size=1, num_gpus=NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'empty anti - run news of himself sexually abu abusing underageageageageageageageageageage states has become comatose in the finance sector . s operations .   hispanics investigating claims , 000 customers in u . s . s assassination . s s operations s operations operations . s terrorists terrorists . s economic agent . s .           .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up temporary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DATA_PATH):\n",
    "    shutil.rmtree(DATA_PATH, ignore_errors=True)\n",
    "if os.path.exists(CACHE_PATH):\n",
    "    shutil.rmtree(CACHE_PATH, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
