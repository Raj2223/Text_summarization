{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4c6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\certifi\\cacert.pem\n"
     ]
    }
   ],
   "source": [
    "import certifi\n",
    "print(certifi.where())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff2c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb366ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 150 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document:\n",
      "\n",
      "    Adani is also among the most controversial of India’s billionaires for his association with the Bharatiya Janata Party (BJP). His close relationship with the party is not coincidental: Adani frequently refers to his business strategy as motivated by “nation building,” which the Adani Group describes on its website as “helping build world-class infrastructure capabilities to help accelerate the growth of India.” Mundra Port and its associated Adani Special Economic Zone, the central components of Adani’s business empire, were attained and developed in cooperation with the Gujarat state government. The BJP led the Gujarat state government during key moments of the Adani Group’s growth, and the relationship resulted in the symbiotic rise of both the BJP and the Adani Group.\n",
      "    \n",
      "\n",
      "BART Summary:\n",
      "Adani is also among the most controversial of India’s billionaires for his association with the Bharatiya Janata Party (BJP) His close relationship with the party is not coincidental. Mundra Port and its associated Adani Special Economic Zone were attained and developed in cooperation with the Gujarat state government.\n",
      "\n",
      "BigBird Pegasus Summary:\n",
      "<s> in this short note we provide a brief overview of thedani family of companies.<n> we start with a brief description of the dani family of companies.<n> then, we provide a brief overview of the state of the art of the industry.<n> we conclude with a summary of the challenges that the industry is facing at the present time. <n> the following is a brief summary of the state of the art of the industry as of march 31, 2011 : <unk>(1 ) number of major shareholders :dani, ii ) major shareholders \n",
      "\n",
      "T5 Summary:\n",
      "Adani is among the most controversial billionaires for his association with the Bharatiya Janata Party (BJP) he frequently refers to his business strategy as motivated by 'nation building' the BJP led the Gujarat state government during key moments of the Adani Group’s growth.\n",
      "\n",
      "Pegasus Summary:\n",
      "Adani is among the most controversial of India’s billionaires for his association with the Bharatiya Janata Party ( BJP)<n>The BJP led the Gujarat state government during key moments of the Adani Group’s growth, and the relationship resulted in the symbiotic rise of both the BJP and the Adani Group.\n",
      "\n",
      "ROUGE Scores for BART:\n",
      "ROUGE-1 Precision: 1.0\n",
      "ROUGE-2 Precision: 0.9583333333333334\n",
      "ROUGE-L Precision: 1.0\n",
      "\n",
      "ROUGE Scores for BigBird Pegasus:\n",
      "ROUGE-1 Precision: 0.26881720430107525\n",
      "ROUGE-2 Precision: 0.010869565217391304\n",
      "ROUGE-L Precision: 0.17204301075268819\n",
      "\n",
      "ROUGE Scores for T5:\n",
      "ROUGE-1 Precision: 0.9772727272727273\n",
      "ROUGE-2 Precision: 0.8837209302325582\n",
      "ROUGE-L Precision: 0.9772727272727273\n",
      "\n",
      "ROUGE Scores for Pegasus:\n",
      "ROUGE-1 Precision: 0.9807692307692307\n",
      "ROUGE-2 Precision: 0.9411764705882353\n",
      "ROUGE-L Precision: 0.9807692307692307\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer, BigBirdPegasusForConditionalGeneration, T5ForConditionalGeneration, T5Tokenizer, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def summarize_text_bart(document):\n",
    "    # Load pre-trained BART model and tokenizer\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Tokenize and encode the document\n",
    "    inputs = tokenizer.encode(\"summarize: \" + document, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_text_bigbird_pegasus(document):\n",
    "    # Load pre-trained BigBird Pegasus model and tokenizer\n",
    "    model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n",
    "\n",
    "    # Tokenize and encode the document\n",
    "    inputs = tokenizer(document, return_tensors='pt')\n",
    "\n",
    "    # Generate summary with necessary parameters\n",
    "    prediction = model.generate(\n",
    "        inputs['input_ids'],  # Provide input IDs explicitly\n",
    "        max_length=120,  # Adjusted for conciseness\n",
    "        num_beams=2,  # Reduced for efficiency\n",
    "        early_stopping=True  # To stop early when possible\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.batch_decode(prediction)[0]\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_text_t5(document):\n",
    "    # Load pre-trained T5 model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "    # Tokenize and encode the document\n",
    "    inputs = tokenizer(\"summarize: \" + document, return_tensors='pt')\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def summarize_text_pegasus(document):\n",
    "    # Load pre-trained Pegasus model and tokenizer\n",
    "    model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "    tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "\n",
    "    # Tokenize and encode the document\n",
    "    inputs = tokenizer(document, return_tensors='pt')\n",
    "\n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "def calculate_rouge_scores(reference_text, generated_summary):\n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_text, generated_summary)\n",
    "\n",
    "    return scores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example document\n",
    "    input_document = \"\"\"\n",
    "    Adani is also among the most controversial of India’s billionaires for his association with the Bharatiya Janata Party (BJP). His close relationship with the party is not coincidental: Adani frequently refers to his business strategy as motivated by “nation building,” which the Adani Group describes on its website as “helping build world-class infrastructure capabilities to help accelerate the growth of India.” Mundra Port and its associated Adani Special Economic Zone, the central components of Adani’s business empire, were attained and developed in cooperation with the Gujarat state government. The BJP led the Gujarat state government during key moments of the Adani Group’s growth, and the relationship resulted in the symbiotic rise of both the BJP and the Adani Group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Summarize the document using different models\n",
    "    bart_summary = summarize_text_bart(input_document)\n",
    "    bigbird_pegasus_summary = summarize_text_bigbird_pegasus(input_document)\n",
    "    t5_summary = summarize_text_t5(input_document)\n",
    "    pegasus_summary = summarize_text_pegasus(input_document)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Original Document:\")\n",
    "    print(input_document)\n",
    "    print(\"\\nBART Summary:\")\n",
    "    print(bart_summary)\n",
    "    print(\"\\nBigBird Pegasus Summary:\")\n",
    "    print(bigbird_pegasus_summary)\n",
    "    print(\"\\nT5 Summary:\")\n",
    "    print(t5_summary)\n",
    "    print(\"\\nPegasus Summary:\")\n",
    "    print(pegasus_summary)\n",
    "\n",
    "    # Compare ROUGE scores for each model\n",
    "    reference_text = input_document\n",
    "    rouge_scores_bart = calculate_rouge_scores(reference_text, bart_summary)\n",
    "    rouge_scores_bigbird_pegasus = calculate_rouge_scores(reference_text, bigbird_pegasus_summary)\n",
    "    rouge_scores_t5 = calculate_rouge_scores(reference_text, t5_summary)\n",
    "    rouge_scores_pegasus = calculate_rouge_scores(reference_text, pegasus_summary)\n",
    "\n",
    "    # Print ROUGE scores\n",
    "    print(\"\\nROUGE Scores for BART:\")\n",
    "    print(\"ROUGE-1 Precision:\", rouge_scores_bart['rouge1'].precision)\n",
    "    print(\"ROUGE-2 Precision:\", rouge_scores_bart['rouge2'].precision)\n",
    "    print(\"ROUGE-L Precision:\", rouge_scores_bart['rougeL'].precision)\n",
    "\n",
    "    print(\"\\nROUGE Scores for BigBird Pegasus:\")\n",
    "    print(\"ROUGE-1 Precision:\", rouge_scores_bigbird_pegasus['rouge1'].precision)\n",
    "    print(\"ROUGE-2 Precision:\", rouge_scores_bigbird_pegasus['rouge2'].precision)\n",
    "    print(\"ROUGE-L Precision:\", rouge_scores_bigbird_pegasus['rougeL'].precision)\n",
    "\n",
    "    print(\"\\nROUGE Scores for T5:\")\n",
    "    print(\"ROUGE-1 Precision:\", rouge_scores_t5['rouge1'].precision)\n",
    "    print(\"ROUGE-2 Precision:\", rouge_scores_t5['rouge2'].precision)\n",
    "    print(\"ROUGE-L Precision:\", rouge_scores_t5['rougeL'].precision)\n",
    "\n",
    "    print(\"\\nROUGE Scores for Pegasus:\")\n",
    "    print(\"ROUGE-1 Precision:\", rouge_scores_pegasus['rouge1'].precision)\n",
    "    print(\"ROUGE-2 Precision:\", rouge_scores_pegasus['rouge2'].precision)\n",
    "    print(\"ROUGE-L Precision:\", rouge_scores_pegasus['rougeL'].precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896dcbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817ac08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a7033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
